# Binary Encodings:

- Joel on Software reading: https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/
    - learned a bit about history of character encoding and that there is a difference between character sets and character encoding. 
    - ASCII => Unicode => ways to encode Unicode that require less space or stricter conventions => UTF-8
    - there is no such thing as plain text, if you are working with strings you must know the encoding and if you are sending strings you must provide the encoding. 

Exploring hellohex

```
$ xxd -g 1 hellohex
00000000: 68 65 6c 6c 6f 20 77 6f 72 6c 64 20 f0 9f 98 80  hello world ....
00000010: 0a
```
converting the first 5 bytes to binary by hand: 

6    8    6    5    6    c 12 6    c 12 6    f 15
8421 8421 8421 8421 8421 8421 8421 8421 8421 8421
0110 1000 0110 0101 0110 1100 0110 1100 0110 1111 

Converting numbers to decimal, storing the remainder of divide by 2 in a stack

4 => 2 => 0010
65 => 32 => 16 => 8 => 4 => 2  100001
105 => 52 => 26 => 13 => 6 => 3 => 1 => 0 1101001
255 => 127 => 63 => 31 => 15 => 7 => 3 => 1 => 0 => 11111111

Converting binary to decimal: 

10 => 2
11 => 3
1101100 => 4 + 8 + 32 + 64 = 108
1010101 => 1 + 4 + 16 + 64 = 85

Binary addition

11111111 
00001101 
100001100

the leading 1 is "carried out" because we only have 8 bits of space, this is called an overflow. 

Two's complement

127 => 01111111
-128 => 1111 1111 => 0000 0001
-1 => 00000001 => 11111110 => 11111111
1 => 00000001
-14 => 00001110 => 11110001 => 11110010

1000 0011 => 0111 1100 => 0111 1101 => 1 + 4 + 8 + 16 + 32 + 64 => -125
1100 0100 => 0011 1011 => 0011 1111 => 1 + 2 + 4 + 8 + 16 + 32 => -60 

01111111
10000000+
11111111

# Byte ordering
$ xxd -b 9001
00000000: 00100011 00101001

$ xxd -g 1 -b tcpheader
00000000: 10101111 00000000 10111100 00000110 01000100 00011110  ....D.
00000006: 01110011 01101000 11101111 11110010 10100000 00000010  sh....
0000000c: 10000001 11111111 01010110 00000000                    ..V.

## Bitmaps
xxd -g 1 image1.bmp
first 14 bytes for the header
42 4d 0a 0e 00 00 00 00 00 00 8a 00 00 00
66 77
BM
size = 

# Floating Point
- essentially scientific notation 
- main advantages are speed and efficiency, can deal with really big numbers and really small numbers without needing huge amounts of space. 

Identify the 3 components of this 32-bit IEEE Floating Point Number and their values.

`01000010001010100000000000000000`

Using the three components, compute the value this represents.

0 First bit = sign = 0 => positive 
1000 0100 exponent => 4 + 128 = 132 - bias which is 127 = 5 so 2^5
0101 0100 0000 0000 0000 000 => I don't get this part about the fraction completely.. 

(0)^2 * (1 + 1/4 + 1/16 + 1/64) * 2^5
1 * 85/64 * 32 = 42.5


# Unicode

1960s - ASCII - 7 bit system => 0 - 127
internet => unicode => hundred of thousand+ characters encoded into a standard (assigned to code points). 
english characters still have their same code points in unicode so if we encoded every character as 32 bits (to cover the range of 100k characters) it would be extremely wasteful because you only need 7 bits for each ascii characters. 
UTF-8 starts with just ascii for the first 128 characters => leading 0 and the normal 7 bits. 
if you need to go above that, you use an encoding scheme where the first few bits are a header indicating the number of bytes and subsequent bytes start with a continuation byte. 

is there any additional space cost to encoding a purely ASCII document as UTF-8?

no, ascii requires 7 bits and utf uses the same code points for ascii meaning an ascii character fits in a byte regardless. 

What are the advantages and detriments of UTF-8 compared to another encoding for Unicode such as UTF-32?

advantages - utf8 uses variable length bytes so there is less waste